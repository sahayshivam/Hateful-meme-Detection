{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('data/hate_memes_train_merged.json', 'r') as r_file:\n",
    "    my_file = json.load(r_file)\n",
    "\n",
    "ids = []\n",
    "img = []\n",
    "label = []\n",
    "text = []\n",
    "\n",
    "for obj in my_file:\n",
    "    ids.append(obj['id'])\n",
    "    img.append(obj['img'])\n",
    "    label.append(obj['label'])\n",
    "    text.append(obj['text'])\n",
    "\n",
    "df = pd.DataFrame({'id': ids, 'img': img,'label': label,'text':text})\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id            img  label  \\\n",
      "0     42953  img/42953.png      0   \n",
      "1     23058  img/23058.png      0   \n",
      "2     13894  img/13894.png      0   \n",
      "3     37408  img/37408.png      0   \n",
      "4     82403  img/82403.png      0   \n",
      "...     ...            ...    ...   \n",
      "8495  10423  img/10423.png      1   \n",
      "8496  98203  img/98203.png      1   \n",
      "8497  36947  img/36947.png      1   \n",
      "8498  16492  img/16492.png      1   \n",
      "8499  15937  img/15937.png      1   \n",
      "\n",
      "                                                   text  \n",
      "0      its their character not their color that matters  \n",
      "1     don't be afraid to love again everyone is not ...  \n",
      "2                              putting bows on your pet  \n",
      "3     i love everything and everybody! except for sq...  \n",
      "4     everybody loves chocolate chip cookies, even h...  \n",
      "...                                                 ...  \n",
      "8495                  nobody wants to hang auschwitz me  \n",
      "8496  when god grants you a child after 20 years of ...  \n",
      "8497  gays on social media: equality! body positivit...  \n",
      "8498  having a bad day? you could be a siamese twin ...  \n",
      "8499  i hate muslims too they take their religion to...  \n",
      "\n",
      "[8500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from google.cloud import vision\n",
    "import os\n",
    "\n",
    "password_file = \"My Project-e17cafef88b1.json\" \n",
    "\n",
    "def get_all(path, password_file=password_file):\n",
    "    \n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = password_file\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "    \n",
    "    \n",
    "    response = client.label_detection(image=image)\n",
    "    labels = response.label_annotations\n",
    "    ret_label = [label.description for label in labels]\n",
    "    print(ret_label)\n",
    "\n",
    "    \n",
    "    objects = client.object_localization(\n",
    "        image=image).localized_object_annotations\n",
    "    ret_object = [(object_.name, object_.score) for object_ in objects]\n",
    "    print(ret_object)\n",
    "\n",
    "    return {'path': path,'labels': ret_label, 'objects': ret_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Forehead', 'Chin', 'Jaw', 'Publication', 'Font', 'Adaptation', 'Poster', 'Photo caption', 'Advertising', 'No expression']\n",
      "[('Person', 0.8108208775520325), ('Person', 0.761947751045227), ('Clothing', 0.6507939696311951)]\n",
      "['Facial expression', 'Smile', 'Flash photography', 'Happy', 'Sleeve', 'Gesture', 'Tie', 'Suit', 'Interaction', 'Font']\n",
      "[('Person', 0.8804235458374023), ('Person', 0.861129105091095), ('Tie', 0.8482463955879211), ('Outerwear', 0.6752802729606628), ('Clothing', 0.5447781682014465)]\n",
      "['Cat', 'White', 'Felidae', 'Carnivore', 'Blue', 'Small to medium-sized cats', 'Whiskers', 'Grey', 'Red', 'Snout']\n",
      "[('Cat', 0.9049119353294373), ('Bowtie', 0.7293491363525391), ('Cat', 0.727033257484436)]\n",
      "['Dog', 'Photograph', 'Light', 'Dog breed', 'Carnivore', 'Black', 'Mammal', 'Companion dog', 'Snout', 'Sporting Group']\n",
      "[('Dog', 0.931591272354126), ('Dog', 0.8330581784248352)]\n",
      "['Musical instrument', 'Font', 'Music', 'Entertainment', 'Event', 'Photo caption', 'Darkness', 'Suit', 'Formal wear', 'Monochrome photography']\n",
      "[('Person', 0.7632495760917664), ('Clothing', 0.557418704032898)]\n",
      "['Shorts', 'Sports uniform', 'World', 'Jersey', 'Gesture', 'Sportswear', 'Player', 'Hat', 'Sports equipment', 'Sneakers']\n",
      "[('Shorts', 0.9330396056175232), ('Shorts', 0.9212886095046997), ('Shorts', 0.9157028198242188), ('Person', 0.8719411492347717), ('Shoe', 0.8680580258369446), ('Person', 0.8604176640510559), ('Person', 0.8593484163284302), ('Shoe', 0.8587496280670166), ('Shorts', 0.8503004908561707), ('Shorts', 0.8370742797851562)]\n",
      "['Ecoregion', 'Masai lion', 'Lion', 'Felidae', 'Big cats', 'Carnivore', 'Grassland', 'Adaptation', 'Mane', 'Plain']\n",
      "[('Lion', 0.9041476845741272), ('Lion', 0.8557741045951843)]\n",
      "['Flash photography', 'Sleeve', 'Gesture', 'Happy', 'Finger', 'Thigh', 'Elbow', 'Street fashion', 'Eyewear', 'Font']\n",
      "[('Person', 0.8154484033584595), ('Bracelet', 0.7217267751693726), ('Top', 0.6721479296684265)]\n",
      "['Dog', 'Carnivore', 'Organism', 'Whiskers', 'Font', 'Fawn', 'Companion dog', 'Happy', 'Dog breed', 'Snout']\n",
      "[('Dog', 0.7552646994590759)]\n",
      "['Hair', 'Photograph', 'Coat', 'Microphone', 'Human', 'Gesture', 'Font', 'Suit', 'News', 'Blazer']\n",
      "[('Person', 0.7917134761810303), ('Person', 0.6438457369804382), ('Person', 0.6250982880592346), ('Necklace', 0.591241717338562)]\n",
      "['Forehead', 'Hairstyle', 'Eyebrow', 'Jaw', 'Publication', 'Poster', 'Font', 'Flash photography', 'Movie', 'Photo caption']\n",
      "[('Clothing', 0.7771877646446228), ('Person', 0.6589775681495667)]\n",
      "['Product', 'Rectangle', 'Font', 'Material property', 'Tints and shades', 'Technology', 'Games', 'Fashion accessory', 'Nail', 'Recreation']\n",
      "[('Packaged goods', 0.8913065791130066), ('Packaged goods', 0.8700392842292786), ('Packaged goods', 0.8456007242202759), ('Packaged goods', 0.7979729175567627), ('Packaged goods', 0.5919008255004883), ('Luggage & bags', 0.5844618082046509), ('Packaged goods', 0.5627064108848572), ('Packaged goods', 0.5246868133544922)]\n",
      "['Microphone', 'Facial expression', 'Beard', 'Happy', 'Facial hair', 'People', 'Spokesperson', 'Moustache', 'Wrinkle', 'Audio equipment']\n",
      "[('Hat', 0.9286600351333618), ('Person', 0.7386595606803894)]\n",
      "['Couch', 'Comfort', 'Fashion', 'Textile', 'Flooring', 'Wood', 'Floor', 'Sharing', 'Living room', 'Leisure']\n",
      "[('Person', 0.8727101683616638), ('Person', 0.8366881608963013), ('Person', 0.7873837351799011), ('Person', 0.7336815595626831), ('Fireplace', 0.6874605417251587), ('Footwear', 0.6263196468353271), ('Couch', 0.6184878945350647), ('Footwear', 0.5024709701538086)]\n",
      "['Face', 'Forehead', 'Hair', 'Smile', 'Skin', 'Lip', 'Photograph', 'Eyebrow', 'Facial expression', 'Eyelash']\n",
      "[('Person', 0.9053186178207397), ('Person', 0.8961262106895447), ('Earrings', 0.8676641583442688), ('Outerwear', 0.8002556562423706), ('Glasses', 0.7102289199829102), ('Person', 0.666845977306366)]\n",
      "['Smile', 'Human', 'Street fashion', 'Textile', 'Happy', 'Pink', 'Crowd', 'Fun', 'Community', 'Magenta']\n",
      "[('Person', 0.8623174428939819), ('Person', 0.8273457884788513), ('Scarf', 0.7830905914306641), ('Jeans', 0.7306967973709106), ('Person', 0.6489587426185608), ('Person', 0.6283605694770813), ('Person', 0.5450601577758789)]\n",
      "['Smile', 'Joint', 'Skin', 'Shoulder', 'Human body', 'Flash photography', 'Fashion', 'Textile', 'Sleeve', 'Happy']\n",
      "[('Person', 0.9314267039299011), ('Overall', 0.8766756653785706), ('Lighting', 0.6339425444602966), ('Furniture', 0.585237443447113)]\n",
      "['Forehead', 'Beard', 'Sleeve', 'Facial hair', 'Collar', 'Moustache', 'Dress shirt', 'Wrinkle', 'Sitting', 'No expression']\n",
      "[('Person', 0.885847806930542), ('Hat', 0.7631920576095581)]\n",
      "['Skin', 'Smile', 'Furniture', 'Human body', 'Comfort', 'Microphone', 'Thigh', 'Spokesperson', 'Happy', 'Event']\n",
      "[('Person', 0.870375394821167), ('Microphone', 0.8575391173362732), ('Couch', 0.7956862449645996), ('Top', 0.6861911416053772)]\n",
      "['Nature', 'Flash photography', 'Standing', 'World', 'Gesture', 'Travel', 'Happy', 'Font', 'Asphalt', 'Street fashion']\n",
      "[('Pants', 0.9112440347671509), ('Outerwear', 0.899703860282898), ('Person', 0.7916072010993958)]\n",
      "['Bird', 'Vertebrate', 'Beak', 'Nature', 'Green', 'Parrot', 'Organism', 'Feather', 'Wing', 'Line']\n",
      "[('Parrot', 0.9310689568519592), ('Parrot', 0.9249864220619202), ('Parrot', 0.9139920473098755), ('Parrot', 0.8728107810020447), ('Bird', 0.7717719078063965)]\n",
      "['Dog', 'Bulldog', 'Dog breed', 'Carnivore', 'Companion dog', 'Fawn', 'Collar', 'Wrinkle', 'Snout', 'Sporting Group']\n",
      "[('Dog', 0.8774392008781433)]\n",
      "['Product', 'Building', 'Sleeve', 'Plant', 'Helmet', 'Wood', 'Real estate', 'Street fashion', 'Font', 'Beauty']\n",
      "[('Person', 0.8765889406204224), ('Person', 0.8471370339393616), ('Outerwear', 0.7956151366233826), ('Pants', 0.789896547794342), ('Pants', 0.667256772518158), ('Jeans', 0.661156177520752)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8b5b75c09509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdicti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6f257feccacd>\u001b[0m in \u001b[0;36mget_all\u001b[0;34m(path, password_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mret_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_helpers/decorators.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(self, image, max_results, retry, timeout, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcopied_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         response = self.annotate_image(\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_helpers/__init__.py\u001b[0m in \u001b[0;36mannotate_image\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         r = self.batch_annotate_images(\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mrequests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_v1/services/image_annotator/client.py\u001b[0m in \u001b[0;36mbatch_annotate_images\u001b[0;34m(self, request, requests, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    823\u001b[0m                  compression=None):\n\u001b[1;32m    824\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[0;32m--> 825\u001b[0;31m                                       wait_for_ready, compression)\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    811\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                 ),), self._context)\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m             \u001b[0m_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "objects = []\n",
    "paths = []\n",
    "for i in range(8499):\n",
    "    path = \"data/\"+str(df['img'][i])\n",
    "    \n",
    "    dicti = get_all(path=path)\n",
    "    labels.append(dicti['labels'])\n",
    "    objects.append(dicti['objects'])\n",
    "for i in range(8499):\n",
    "    \n",
    "    paths.append(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for i in range(len(labels)):\n",
    "    path = \"data/\"+str(df['img'][i])\n",
    "    paths.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels = []\n",
    "for lb in labels:\n",
    "    if lb in ['Photo caption', 'Photography', 'Font', 'Text', 'Internet meme']:\n",
    "        pass\n",
    "    else:\n",
    "        list_labels.append(lb)\n",
    "labels.append(list_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(objects)):\n",
    "    for j in range(len(objects[i])):\n",
    "        if objects[i][j][1]>0:\n",
    "            \n",
    "            list_objects.append(objects[i][j][0])\n",
    "list_objects = set(list_objects)\n",
    "list_objects = list(list_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame({'path': paths,'text': text[:23], 'labels': labels[:23], 'objects':list_objects[:23]\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.merge(my_df, left_on='img', right_on='path')\n",
    "new_df = new_df.rename(columns={'text_x':'text', 'text_y':'ext_text'})\n",
    "new_df.drop('path', axis=1, inplace=True)\n",
    "new_df['objects'] = new_df['objects'].apply(set).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sohamjain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_elong_text'] = df['text'].apply(lambda x: replaceElongated(x))\n",
    "new_df['no_elong_text'] = new_df['text'].apply(lambda x: replaceElongated(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_slang'] = df['no_elong_text'].apply(lambda x: replaceSlang(x))\n",
    "new_df['no_slang'] = new_df['no_elong_text'].apply(lambda x: replaceSlang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sohamjain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4cb57a9a0281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtfid1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2131\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1814\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1815\u001b[0m         )\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve, classification_report, plot_confusion_matrix, plot_roc_curve\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "new_df = new_df\n",
    "X = new_df[['no_slang']]\n",
    "\n",
    "y = np.array(new_df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "tfid1 = TfidfVectorizer(stop_words=stopwords.words('english'), min_df=25)\n",
    "X1 = tfid1.fit_transform(X_train['no_slang'])\n",
    "X11 = tfid1.transform(X_test['no_slang'])\n",
    "X_text_test = pd.DataFrame(X11.toarray())\n",
    "X_text = pd.DataFrame(X1.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a1114958a179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pickle.dump(model, open(\"logistic.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "pickle.dump(model,open(\"nai.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pickle.dump(model, open(\"ada.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(min_samples_leaf=10, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "pickle.dump(model, open(\"rfc.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
